# Offline Evaluation Specification (User-AI Simulation)

## 1. Purpose

Define a lightweight **offline** evaluation pipeline for chat quality where a **User AI simulator** talks to a pre-prod AI bot, conversations are traced, and sessions are scored.

This complements the existing online evaluation pipeline (`online/`) that fetches real traces from Langfuse.

## 2. What "Offline" Means

For this repository, **offline evaluation** means:

- No real end-user traffic (synthetic traffic only, generated by User AI).
- Runs execute in non-production environments.
- Simulated conversations are first-class traces (typically visible in Langfuse through existing LLM/chat integrations).
- Traces and scores remain isolated from prod via environment filters and dedicated user-id prefix.
- Reproducible scenario selection and run settings remain required.

Operationally, this can still use the same online-style contour (Langfuse fetch + evaluate + Langfuse score push); "offline" refers to traffic origin, not the absence of observability systems.

## 3. Scope

### In scope

- Simulated user-bot conversations.
- Scenario-driven test coverage.
- Langfuse-native scoring and summary output.
- Cheap deterministic metrics + optional LLM-judge metrics.

### Out of scope (initial version)

- Heavy infrastructure (queues, databases, orchestration systems).

## 4. Design Principles

- Lightweight: minimal files, fast local runs.
- Reproducible: seed-controlled randomness.
- Comparable: reuse existing quality metric definitions where possible.
- Pluggable: simulator and bot adapter can be swapped independently.

## 5. Proposed Folder and Files

Add a new `offline/` package:

- `offline/scenarios.jsonl`
- `offline/user_simulator.py`
- `offline/bot_adapter.py`
- `offline/runner.py`
- `offline/scorer.py`
- `offline/main.py`
- `offline/results/` (generated outputs)

## 6. Components

### 6.1 `offline/scenarios.jsonl`

Input dataset for simulated sessions (one JSON object per line).

Required fields:

- `id` (string)
- `persona` (string)
- `goal` (string)
- `constraints` (array of strings)
- `max_turns` (int)

Optional fields:

- `must_include` (array of strings)
- `must_avoid` (array of strings)

Example:

```json
{"id":"billing_refund_001","persona":"impatient customer","goal":"get refund eligibility and next steps","constraints":["does not know order id","is upset"],"max_turns":8,"must_include":["refund policy","required info"],"must_avoid":["hallucinated promises"]}
```

### 6.2 `offline/user_simulator.py`

`UserSimulator.next_message(state, bot_reply) -> str`

Relevant LLM (e.g., from Replicate) for natural multi-turn behavior.

Responsibilities:

- Maintain scenario intent.
- React to bot replies.
- Stop when goal reached, blocked, or max turns exhausted.

### 6.3 `offline/bot_adapter.py`

Single interface to the bot under test:

- `reply(messages: list[dict[str, str]]) -> str`

Adapter implementations can support:

- Direct Python callable (local bot SDK/module).
- HTTP endpoint (local/staging bot service).

### 6.4 `offline/runner.py`

Core conversation loop per scenario:

1. Generate initial user message from scenario.
2. Alternate `user -> bot`.
3. Track transcript, timing, turn count, stop reason.
4. Persist per-session artifact in `offline/results/`.

Run controls:

- `--n` sample count
- `--seed` reproducibility
- `--max-turns-override` optional global limit

### 6.5 `offline/scorer.py`

Score each transcript with:

- Existing LLM quality metrics (1-5):
  - relevance
  - engagement
  - naturalness
  - appropriateness
- Derived:
  - `composite_score` (sum, 4-20)

### 6.6 `offline/main.py`

CLI entrypoint:

```bash
python -m evaluation.offline.main --scenarios offline/scenarios.jsonl --n 50 --seed 42
```

Expected output:

- Console summary (aggregate metrics, pass rates).
- JSON report for all sessions.
- Optional per-session transcript files.

## 7. Data Contracts

### 7.1 Session result (JSON)

```json
{
  "run_id": "20260226_174500",
  "scenario_id": "billing_refund_001",
  "seed": 42,
  "turns": [
    {"role": "user", "content": "..."},
    {"role": "assistant", "content": "..."}
  ],
  "stop_reason": "goal_reached",
  "metrics": {
    "relevance": 4,
    "engagement": 3,
    "naturalness": 4,
    "appropriateness": 5,
    "composite_score": 16
  }
}
```

### 7.2 Aggregate report (JSON)

- total sessions
- success/failure counts
- mean/min/max for numeric metrics

## 8. Execution Flow

1. Load scenarios.
2. Select scenarios (`--n`, seed).
3. Simulate conversations with `UserSimulator` + `BotAdapter`.
4. Score each conversation.
5. Save session artifacts + aggregate report.
6. Print concise summary.

## 9. Integration With Existing `online/` Code

- Reuse metric names and score ranges from `online/judge_llm.py`.
- Keep offline and online entrypoints separate.
- Reuse the online evaluation contour with offline filters/settings:
  - fetch traces from Langfuse (non-prod env + offline user-id prefix)
  - run Judge LLM scoring
  - push scores back to Langfuse
- Net-new component remains the User AI simulator that generates synthetic sessions.

## 10. Acceptance Criteria (MVP)

- `python -m evaluation.offline.main ...` runs against non-prod environment(s).
- At least one scenario file can produce complete transcripts.
- Reports include all listed metrics.
- Runs are reproducible with the same seed.
- Runtime remains lightweight for local development (e.g., 50 sessions).
- Offline sessions are isolated in Langfuse using dedicated user-id prefix (e.g., `offline-eval-service-*`).

## 11. Future Extensions

- Stronger goal-checking with semantic matching.
- Persona-level breakdown dashboards.
- Regression gate thresholds for CI.
- A/B bot comparison on identical scenario sets.

## 12. Further Iteration Ideas (Langfuse-Native Offline Mode)

- Treat simulated sessions as first-class traces in Langfuse (already natural when chat calls go through LLM APIs with Langfuse integration).
- Use Langfuse as the system of record for offline runs:
  - read evaluation traces from Langfuse
  - write evaluation scores back to Langfuse
- Keep isolation strict:
  - use a dedicated user id prefix such as `offline-eval-service-*`
  - run in non-production environment(s) only
- Reuse the existing online evaluation contour with offline settings and filters; the only net-new component is the `User AI` simulator that generates conversations.
- Main benefit: offline and online/prod share dashboards, plots, and alerts, enabling direct trend comparison with minimal new observability work.
